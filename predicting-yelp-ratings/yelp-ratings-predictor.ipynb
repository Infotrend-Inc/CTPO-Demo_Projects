{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 align=\"center\">Yelp Rating Prediction</h1>\n",
    "\n",
    "In this Jupyter Notebook project, a suspected correlation is explored between the text content of a business' user reviews and the business' respective star rating. By applying natural language processing (NLP) to the text content of user reviews, a supervised machine learning model in [TensorFlow](https://www.tensorflow.org/) can be constructed to predict star ratings (i.e. $f(reviews)=star\\ rating$). Despite discrete star rating values (e.g. $[1.0, 1.5, ..., 4.5, 5.0]$), a regression model is used here instead of classification since the star ratings are within a numerical range.\n",
    "\n",
    "The dataset used for this project is sourced from the [Yelp Open Dataset](https://www.yelp.com/dataset), a public and non-commercial dataset for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Additional Library Installation\n",
    "\n",
    "To enhance the functionality of the CTPO environment, some additional libraries not pre-installed may be required. Follow these steps to install the necessary libraries from the `requirements.txt` file:\n",
    "\n",
    "#### Create and Activate the Virtual Environment:\n",
    "1. Open your terminal or command prompt within the Jupyter notebook: `File -> New -> Terminal`. Type `bash` to obtain a shell that is compatible with some of the commands below.\n",
    "2. Navigate to the project directory:\n",
    "   ```bash\n",
    "   cd /path/to/your/project/directory\n",
    "   pwd\n",
    "3. Execute the following commands to create and activate the virtual environment\n",
    "   ```bash\n",
    "   python3 -m venv --system-site-packages myvenv\n",
    "   source myvenv/bin/activate\n",
    "   pip3 install ipykernel\n",
    "   python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "   ```\n",
    "4. Before running the following commands, we recommend you load the \"Python (myvenv)\" kernel in order to limit the chances of altering the underlying container and then make sure you are in the directory where the Jupyter Notebook and the `myvenv` directory are located. This ensures the `./` path is always current. You can use the `cd` command to change to your project directory and `pwd` to verify your current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! . ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Python Imports\n",
    "All relevant project libraries and utilities for the notebook are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, List\n",
    "from statistics import mean, stdev\n",
    "from io import StringIO\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "import json  \n",
    "import math\n",
    "import pandas\n",
    "import numpy\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as pyplot\n",
    "import spectra\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# store standard out stream\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas {pandas.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "print(f\"Tensor Flow Version: {tensorflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configure PRNG Seed\n",
    "Numpy, Tensorflow, and SciKit rely on [pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) for internal operations. To guarantee deterministic behaviour and reproducible results when executing this notebook, an arbitrary seed is configured and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# arbitrary changeable seed constant\n",
    "PRNG_SEED = 42\n",
    "\n",
    "numpy.random.seed(PRNG_SEED)\n",
    "tensorflow.random.set_seed(PRNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# I. Data Importation\n",
    "The Yelp data needs to be imported from its JSON file format into usable in-memory data structures. Yelp provides documentation on their dataset file formats and structures [here](https://www.yelp.com/dataset/documentation/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset Location\n",
    "For this project, all data is stored in the `data` folder in the repository root directory. Historically, Yelp has changed the file names a few times; thus, they are enumerated below at the time of implementing this project. A few path constants are set here and used later when importing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!dir \"./data/\"\n",
    "\n",
    "FILE_BUSINESSES = r\"data/yelp_academic_dataset_business.json\"\n",
    "FILE_USER_REVIEWS = r\"data/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Yelp Businesses\n",
    "All public registered businesses are loaded from the new-line (e.g. `\\n`) delimited JSON file and are indexed into a dictionary by their corresponding `business_id`. Of only particular interest, are the business' `business_id`, `review_count`, and `stars` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# business data structure\n",
    "class Business(TypedDict):\n",
    "    business_id: str\n",
    "    review_count: int\n",
    "    stars: float\n",
    "\n",
    "# businesses indexed by business_id (i.e. {business['business_id']: Business}\n",
    "businesses_by_id: Dict[str, Business] = {}\n",
    "\n",
    "# parse all businesses\n",
    "with open(FILE_BUSINESSES, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-delimited JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        business: Business = json.loads(record)\n",
    "        # map Business by business_id\n",
    "        businesses_by_id[business['business_id']] = business\n",
    "\n",
    "# metrics\n",
    "print(f\"Imported {len(businesses_by_id):,} distinct businesses in {time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import User Reviews\n",
    "All public user reviews of businesses are loaded from the new-line (e.g. `\\n`) delimited JSON file and are indexed into a dictionary array by matching `business_id`s. Of only particular interest, are the business' `business_id` and `text` attributes - no other data from user reviews is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# user review data structure\n",
    "class UserReview(TypedDict):\n",
    "    business_id: str\n",
    "    text: str\n",
    "\n",
    "# user reviews indexed by business_id (i.e. {business_id: UserReview['text'][]})\n",
    "business_review_texts: Dict[str, List[str]] = {\n",
    "    business_id: []\n",
    "    for business_id in businesses_by_id.keys()\n",
    "}\n",
    "\n",
    "# parse user reviews\n",
    "with open(FILE_USER_REVIEWS, 'r', encoding='utf-8') as file:\n",
    "    # iterate over newline-delimited JSON records\n",
    "    record: str\n",
    "    for record in file:\n",
    "        # parse JSON record\n",
    "        review: UserReview = json.loads(record)\n",
    "        # map user review by business_id\n",
    "        business_review_texts[review['business_id']].append(review[\"text\"])\n",
    "\n",
    "# metrics\n",
    "print(f\"Imported {sum([len(reviews) for reviews in business_review_texts.values()]):,} user reviews in {time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## II. Dataset Visualization\n",
    "\n",
    "Before training the machine learning model, some of the business attributes should be explored better investigate the dataset and understand its distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Star Rating Distribution\n",
    "The following graphic displays how discrete star ratings (i.e. $[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]$) for businesses on Yelp are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "pyplot.figure(figsize=(12, 4))\n",
    "pyplot.title(\"Star Rating Histogram\")\n",
    "pyplot.xlabel(\"Star Rating\")\n",
    "pyplot.ylabel(\"Frequency\")\n",
    "star_color = spectra.scale(['red', 'yellow', 'green'])\n",
    "for star_rating, frequency in pandas.DataFrame(businesses_by_id.values(), columns=['stars'])['stars'].value_counts().sort_index().items():\n",
    "    pyplot.bar(str(star_rating), frequency, color=star_color((star_rating - 1)/ 4).hexcode)\n",
    "    pyplot.text(star_rating * 2 - 2.25, frequency + 400, frequency)\n",
    "pyplot.show()\n",
    "\n",
    "# metrics\n",
    "print(f\"Rendered graphic in {time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Review Count Distribution\n",
    "The following graphic displays a frequency count and cumulative distribution of review counts for businesses on Yelp.\n",
    "\n",
    "It becomes quite obvious our dataset is skewed and that there might be dataset noise and significant biases at play when trying to find a correlation between user reviews and a business' star rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "review_counts = [business['review_count'] for business in businesses_by_id.values()]\n",
    "figure = pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "axes = figure.add_subplot(1, 2, 1)\n",
    "axes.set_title(\"Frequency of Review Counts\")\n",
    "axes.set_xlabel(\"Business Review Count\")\n",
    "axes.axis(xmin=0, xmax=mean(review_counts) + stdev(review_counts))\n",
    "axes.set_ylabel(\"Frequency\")\n",
    "axes.grid(True)\n",
    "axes.hist(review_counts, bins = list(range(0, max(review_counts))))\n",
    "\n",
    "axes = figure.add_subplot(1, 2, 2)\n",
    "axes.set_title(\"Cumulative Distribution of Review Counts\")\n",
    "axes.set_xlabel(\"Business Review Count\")\n",
    "axes.axis(xmin=0, xmax=mean(review_counts) + stdev(review_counts))\n",
    "axes.set_ylabel(\"Cumulative Proportion\")\n",
    "axes.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "axes.grid(True)\n",
    "pyplot.hist(review_counts, bins=list(range(0, max(review_counts))), cumulative=True, density=True)\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "# metrics\n",
    "print(f\"Rendered graphic in {time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## III. Data Preprocessing\n",
    "Before training a machine learning model, the raw text content has to be preprocessed into numerical matrices for TensorFlow. In the following subsections, the dataset is prepared for the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Filter Businesses with Less than 15 Reviews\n",
    "As seen in the visualization of star ratings, a significant amount of businesses do not have many user reviews. This is problematic, creating strong biases and word associations in user reviews on less-reviewed businesses. To mitigate bias and variance, a business must have a minimum threshold of 15 or more user reviews to be considered for the remainder of the project. The results should still be statistically significant, as close to half of all businesses have 15 or more reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# minimum business['review_count'] required to be selected\n",
    "MINIMUM_REVIEW_COUNT = 15\n",
    "\n",
    "selected_businesses = [business for business in businesses_by_id.values() if MINIMUM_REVIEW_COUNT <= business['review_count']]\n",
    "\n",
    "print(f\"Selected {len(selected_businesses):,} businesses (filtered {len(businesses_by_id) - len(selected_businesses):,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Partitioning: Training, Validation, and Testing\n",
    "It is rather computationally expensive to use the entire dataset; instead a subset of the selected businesses are split into 3 disjoint partitions:\n",
    "- **training**: used for training the model\n",
    "- **validation**: provides an evaluation of the model fit during training\n",
    "- **testing**: provides an evaluation of the final fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# partition size constants\n",
    "TRAINING_SIZE = 25_000\n",
    "VALIDATE_SIZE = 1_000\n",
    "TESTING_SIZE = 10_000\n",
    "\n",
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# perform partitions; note: train_test_split does not support 3-way splitting\n",
    "partitions = {}\n",
    "partitions['train'], _ = train_test_split(\n",
    "    # select all businesses that are greater than or equal to MINIMUM_REVIEW_COUNT\n",
    "    selected_businesses,\n",
    "    # partition proportions\n",
    "    train_size=TRAINING_SIZE,\n",
    "    shuffle=True,\n",
    "    # PRNG seed for deterministic behaviour\n",
    "    random_state=PRNG_SEED,\n",
    ")\n",
    "partitions['validate'], partitions['test'] = train_test_split(\n",
    "    # select all businesses not in partitions['train']\n",
    "    [\n",
    "        businesses_by_id[business_id]\n",
    "        for business_id in set([business['business_id'] for business in selected_businesses]).difference(set([business['business_id'] for business in partitions['train']]))\n",
    "    ],\n",
    "    # partition proportions\n",
    "    train_size=VALIDATE_SIZE,\n",
    "    test_size=TESTING_SIZE,\n",
    "    shuffle=True,\n",
    "    # PRNG seed for deterministic behaviour\n",
    "    random_state=PRNG_SEED,\n",
    ")\n",
    "\n",
    "# metrics\n",
    "print(f\"Partitioned {len(businesses_by_id):,} businesses into { {category: len(partition) for category, partition in partitions.items()}} in {time() - start_time:.6f} seconds\")\n",
    "print(f\"Remaining unused businesses: {len(businesses_by_id) - sum([len(partition) for partition in partitions.values()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Input Features: Preprocess Review Texts using TF-IDF Vectorization\n",
    "Natural language processing must be applied onto the user reviews to prepare the natural language input data into numerical matrices. Concatenated user reviews are vectorized into a [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighted word matrix.\n",
    "\n",
    "An important note is that this operation mitigates bias by splitting the training, validation, and testing subsets prior to fitting the TF-IDF vectorization model. Training data is used to fit the model for vectorization features and is later transformed, while validation testing data is _only_ transformed by the model.\n",
    "\n",
    "> **Note**: Increasing the `max_features` vectorizer parameter decreases the resulting RMSE score, but with diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# construct vectorizer\n",
    "vectorizer: TfidfVectorizer = TfidfVectorizer(\n",
    "    # maximum word features\n",
    "    max_features=1000,\n",
    "    # prune english stop words\n",
    "    stop_words='english',\n",
    ")\n",
    "\n",
    "# construct corpus partitions\n",
    "tf_input = {\n",
    "    category: getattr(vectorizer, 'fit_transform' if category == 'train' else 'transform')\n",
    "              (['\\n'.join(business_review_texts[business['business_id']]) for business in partition])\n",
    "              .toarray()\n",
    "    for category, partition in partitions.items()\n",
    "}\n",
    "\n",
    "# metrics\n",
    "print(f\"Word features cardinality: {len(vectorizer.get_feature_names_out()):,}\")\n",
    "print(f\"Word features: {vectorizer.get_feature_names_out()}\")\n",
    "print(f\"IDF Vectorized {sum([len(matrix) for matrix in tf_input.values()]):,} businesses review texts in {time() - start_time:.6f} seconds\")\n",
    "print()\n",
    "print(f\"Training Shape: {tf_input['train'].shape}\")\n",
    "print(f\"Validate Shape: {tf_input['validate'].shape}\")\n",
    "print(f\"Testing Shape: {tf_input['test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Output Features: Select Star Rating\n",
    "The only output feature for our training model is a business' unnormalized star rating. Despite the discrete values for star ratings, regression is used here and the predicted result can be rounded to the nearest half star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_output = {\n",
    "    category: numpy.array([[business['stars']] for business in partition])\n",
    "    for category, partition in partitions.items()\n",
    "}\n",
    "\n",
    "print(f\"Training Shape: {tf_output['train'].shape}\")\n",
    "print(f\"Validate Shape: {tf_output['validate'].shape}\")\n",
    "print(f\"Testing Shape: {tf_output['test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# IV. Model Training\n",
    "Using the defined input and output feature matrices, a neural network can be trained with a supervised learning regression model. Potentially, a correlation may be discovered between text reviews and the star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Model\n",
    "A supervised learning Sequential neural network is constructed for building a regression model. A simple model with fully-connected dense layers is employed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, input_dim=tf_input['train'].shape[1]),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ],\n",
    "    name=\"yelp_regression\"\n",
    ")\n",
    "# set optimizer for gradient descent\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mse'])\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# store standard out stream\n",
    "original_stdout = sys.stdout\n",
    "# replace standard out for intercepting model training logs\n",
    "sys.stdout = captured_stdout = StringIO()\n",
    "\n",
    "try:\n",
    "    # make temporary directory for saved model\n",
    "    if not os.path.exists(\"temp\"):\n",
    "        os.mkdir(\"temp\")\n",
    "\n",
    "    # perform model training\n",
    "    model.fit(\n",
    "        # training data\n",
    "        tf_input['train'], tf_output['train'],\n",
    "        # use test data to validate losses, but not for training\n",
    "        validation_data=(tf_input['validate'], tf_output['validate']),\n",
    "        callbacks=[\n",
    "            # patience: number of epochs with no improvement after which training will be stopped\n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, mode='auto', verbose=0),\n",
    "            # save best model from all epochs\n",
    "            ModelCheckpoint(filepath=\"temp/model_best_weights.keras\", save_best_only=True, verbose=0)\n",
    "        ],\n",
    "        verbose=2,\n",
    "        epochs=1000,\n",
    "    )\n",
    "finally:\n",
    "    # restore standard out\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# metrics\n",
    "print(f\"Model trained in {time() - start_time:.6f} seconds\")\n",
    "# noinspection PyTypeChecker\n",
    "display(HTML(f'\\\n",
    "    <details>\\\n",
    "        <summary><b>Training Details</b></summary>\\\n",
    "        <sub><sup>\\\n",
    "            {\"<br>\".join(captured_stdout.getvalue().splitlines())}\\\n",
    "        </sup></sub>\\\n",
    "    </details>\\\n",
    "'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# V. Model Accuracy\n",
    "The accuracy of the model can be quantified and visualized by comparing the predicted outputs with the actual expected outputs with the testing subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Predictions\n",
    "Using the testing subset, predicted star ratings can be quickly computed with our trained model and its weights. Model predictions are computed using the regression model as continuous values and then rounded to the nearest half star as discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "start_time = time()\n",
    "\n",
    "# load best model for prediction\n",
    "model.load_weights(\"temp/model_best_weights.keras\")\n",
    "\n",
    "# compute predicted ratings for test dataset\n",
    "predicted = model.predict(tf_input['test'])\n",
    "# map predictions to the nearest half-star\n",
    "predicted_rounded = numpy.array([min(round(prediction * 2) / 2, 5) for prediction in predicted.flatten()])\n",
    "\n",
    "# metrics\n",
    "print(f\"Predicted {len(tf_input['test']):,} business ratings in {time() - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Error Scoring (MSE/RSME)\n",
    "Our model's prediction accuracy can be quantified numerically with a [MSE or RMSE](https://en.wikipedia.org/wiki/RSME) score. The score is actually quite statistically significant - the deviation is within the realm of roughly a half star. The scores are shown below for continuous predictions and discrete half-star rounded predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse_score = sklearn.metrics.mean_squared_error(predicted, tf_output['test'])\n",
    "mse_rounded_score = sklearn.metrics.mean_squared_error(predicted_rounded, tf_output['test'])\n",
    "\n",
    "pandas.DataFrame(\n",
    "    [\n",
    "        [mse_score, mse_rounded_score],\n",
    "        numpy.sqrt([mse_score, mse_rounded_score])\n",
    "    ],\n",
    "    columns=[\"Continuous\", \"Discrete\"],\n",
    "    index=[\"MSE\", \"RMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lift Chart Visualization\n",
    "Quantified error scoring does not lend itself to be easily interpreted; instead, the results may be visualized using a lift chart visualization from our predictions and actual values. Arbitrary businesses are ordered by their actual star rating; the deviations from the predicted value are quickly visible from the actual expected values.\n",
    "\n",
    "Interestingly enough, since a regression model was used here, continuous values above a 5 star rating are predicted. For the discrete visualization, values are clamped to only possible values for star ratings (i.e. clamped to `[0, 5]` and rounded to the nearest half-star)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure = pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "axes = figure.add_subplot(1, 2, 1)\n",
    "axes.set_title(\"Continuous Predictions\")\n",
    "axes.set_xlabel(\"businesses, ordered by ascending star rating\")\n",
    "axes.set_ylabel(\"star rating \")\n",
    "table = pandas.DataFrame({'predicted': predicted.flatten(), 'y': tf_output['test'].flatten()})\n",
    "table.sort_values(by=['y'], inplace=True)\n",
    "axes.plot(table['predicted'].tolist(), label=\"prediction\")\n",
    "axes.plot(table['y'].tolist(), label=\"expected\")\n",
    "axes.legend()\n",
    "\n",
    "axes = figure.add_subplot(1, 2, 2)\n",
    "axes.set_title(\"Discrete Predictions\")\n",
    "axes.set_xlabel(\"businesses, ordered by ascending star rating\")\n",
    "axes.set_ylabel(\"star rating\")\n",
    "table = pandas.DataFrame({'predicted': predicted_rounded.flatten(), 'y': tf_output['test'].flatten()})\n",
    "table.sort_values(by=['y'], inplace=True)\n",
    "axes.plot(table['predicted'].tolist(), label=\"prediction\")\n",
    "axes.plot(table['y'].tolist(), label=\"expected\")\n",
    "axes.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<hr style=\"border-top: 2px solid #444\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# VI. Conclusion\n",
    "\n",
    "If a business has enough reviews (in our case, 15), there is a surprisingly accurate correlation between the text content of the user reviews and the star rating the business receives. The discrete lift chart visualization shows most of the deviation of predicted values occurs within a half-star of the actual expected value.\n",
    "\n",
    "The purpose of this project was not to optimize a model's ability to accurate predict stars, but to inspect a suspected correlation of data. It may be possible to construct a significantly better regression model using more input features of businesses (such as location, hours of operation, tips, etc), but optimization for this project is not quite necessary for identifying a correlation with a business's user reviews and its respective star rating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
